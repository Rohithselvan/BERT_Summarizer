{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11226604,"sourceType":"datasetVersion","datasetId":7011918}],"dockerImageVersionId":31194,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install --upgrade pip","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T11:56:57.195288Z","iopub.execute_input":"2025-11-08T11:56:57.196170Z","iopub.status.idle":"2025-11-08T11:57:02.962387Z","shell.execute_reply.started":"2025-11-08T11:56:57.196142Z","shell.execute_reply":"2025-11-08T11:57:02.961460Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (24.1.2)\nCollecting pip\n  Downloading pip-25.3-py3-none-any.whl.metadata (4.7 kB)\nDownloading pip-25.3-py3-none-any.whl (1.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pip\n  Attempting uninstall: pip\n    Found existing installation: pip 24.1.2\n    Uninstalling pip-24.1.2:\n      Successfully uninstalled pip-24.1.2\nSuccessfully installed pip-25.3\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"pip install datasets","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-08T11:57:04.724475Z","iopub.execute_input":"2025-11-08T11:57:04.725282Z","iopub.status.idle":"2025-11-08T11:57:10.818932Z","shell.execute_reply.started":"2025-11-08T11:57:04.725249Z","shell.execute_reply":"2025-11-08T11:57:10.818093Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (4.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.20.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\nCollecting pyarrow>=21.0.0 (from datasets)\n  Downloading pyarrow-22.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\nRequirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.4.0)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.5)\nRequirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.6.0)\nRequirement already satisfied: multiprocess<0.70.19 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.18)\nRequirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.10.0)\nRequirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.36.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.3)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (4.11.0)\nRequirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (2025.10.5)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (1.0.9)\nRequirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (3.11)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.2.0)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2022.3.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nDownloading pyarrow-22.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (47.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pyarrow\n  Attempting uninstall: pyarrow\n    Found existing installation: pyarrow 19.0.1\n    Uninstalling pyarrow-19.0.1:\n      Successfully uninstalled pyarrow-19.0.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed pyarrow-22.0.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"pip install nltk","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T11:57:13.669607Z","iopub.execute_input":"2025-11-08T11:57:13.670410Z","iopub.status.idle":"2025-11-08T11:57:15.560148Z","shell.execute_reply.started":"2025-11-08T11:57:13.670372Z","shell.execute_reply":"2025-11-08T11:57:15.559368Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.2)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.3.0)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.2)\nRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2025.11.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import os\nimport tarfile\nimport nltk\nfrom datasets import Dataset, DatasetDict\nfrom sklearn.model_selection import train_test_split\nfrom transformers import BertTokenizer\n\n# Download NLTK data\nnltk.download('punkt')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T11:57:17.848771Z","iopub.execute_input":"2025-11-08T11:57:17.849376Z","iopub.status.idle":"2025-11-08T11:57:26.133930Z","shell.execute_reply.started":"2025-11-08T11:57:17.849343Z","shell.execute_reply":"2025-11-08T11:57:26.133294Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"# Step 2: Extract dataset files\nimport tarfile\nimport os\n\n# Path to the .tgz file in Kaggle environment\ncnn_stories_path = \"/kaggle/input/nlp-proj/cnn_stories.tgz\"\n# Directory where you want to extract the files\nextraction_path = \"/kaggle/working/cnn_stories\"  # This is the standard working directory in Kaggle\n\n# Ensure the extraction directory exists\nos.makedirs(extraction_path, exist_ok=True)\n\n# Open the .tgz file\nwith tarfile.open(cnn_stories_path, 'r:gz') as tar:\n    # Get all files in the archive\n    all_files = tar.getnames()\n    \n    files_to_extract = all_files\n    \n    # Extract the selected files\n    tar.extractall(path=extraction_path, members=[tar.getmember(file) for file in files_to_extract])\n\nprint(f\"Extracted the files to {extraction_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T11:57:31.478843Z","iopub.execute_input":"2025-11-08T11:57:31.479273Z","iopub.status.idle":"2025-11-08T12:00:50.475375Z","shell.execute_reply.started":"2025-11-08T11:57:31.479254Z","shell.execute_reply":"2025-11-08T12:00:50.474573Z"}},"outputs":[{"name":"stdout","text":"Extracted the files to /kaggle/working/cnn_stories\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Step 3: Define the story parser\ndef parse_story_file(file_path):\n    with open(file_path, 'r', encoding='utf-8') as f:\n        content = f.read()\n        parts = content.split('@highlight')\n        article = parts[0].strip()\n        highlights = [h.strip() for h in parts[1:] if h.strip()]\n        highlights = ' '.join(highlights)\n        return {'article': article, 'highlights': highlights}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T12:01:12.474162Z","iopub.execute_input":"2025-11-08T12:01:12.474697Z","iopub.status.idle":"2025-11-08T12:01:12.479376Z","shell.execute_reply.started":"2025-11-08T12:01:12.474673Z","shell.execute_reply":"2025-11-08T12:01:12.478658Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Step 4: Process all CNN stories\ndef process_stories(story_dir):\n    data = []\n    for root, _, files in os.walk(story_dir):\n        for file in files:\n            if file.endswith('.story'):\n                try:\n                    data.append(parse_story_file(os.path.join(root, file)))\n                except Exception as e:\n                    print(f\"Error processing {file}: {e}\")\n    return data\n\nprint(\"Processing CNN stories...\")\ncnn_data = process_stories('cnn_stories/cnn/stories')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T12:01:39.803335Z","iopub.execute_input":"2025-11-08T12:01:39.803938Z","iopub.status.idle":"2025-11-08T12:01:43.186839Z","shell.execute_reply.started":"2025-11-08T12:01:39.803913Z","shell.execute_reply":"2025-11-08T12:01:43.186199Z"}},"outputs":[{"name":"stdout","text":"Processing CNN stories...\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Step 5: Create train/val/test splits\ntrain_data, temp_data = train_test_split(cnn_data, test_size=0.2, random_state=42)\nval_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T12:02:03.583425Z","iopub.execute_input":"2025-11-08T12:02:03.583930Z","iopub.status.idle":"2025-11-08T12:02:03.615228Z","shell.execute_reply.started":"2025-11-08T12:02:03.583904Z","shell.execute_reply":"2025-11-08T12:02:03.614523Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Step 6: Create HuggingFace Dataset\ndataset = DatasetDict({\n    'train': Dataset.from_list(train_data),\n    'validation': Dataset.from_list(val_data),\n    'test': Dataset.from_list(test_data)\n})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T12:02:15.842370Z","iopub.execute_input":"2025-11-08T12:02:15.842664Z","iopub.status.idle":"2025-11-08T12:02:19.698590Z","shell.execute_reply.started":"2025-11-08T12:02:15.842641Z","shell.execute_reply":"2025-11-08T12:02:19.697791Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Step 7: Initialize tokenizer and preprocess\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\ndef preprocess_function(batch):\n    inputs = tokenizer(\n        batch[\"article\"],\n        max_length=512,\n        truncation=True,\n        padding=\"max_length\"\n    )\n\n    outputs = tokenizer(\n        batch[\"highlights\"],\n        max_length=128,\n        truncation=True,\n        padding=\"max_length\"\n    )\n\n    batch[\"input_ids\"] = inputs.input_ids\n    batch[\"attention_mask\"] = inputs.attention_mask\n    batch[\"labels\"] = outputs.input_ids\n\n    # Replace padding token id with -100 for loss calculation\n    batch[\"labels\"] = [\n        [-100 if token == tokenizer.pad_token_id else token for token in labels]\n        for labels in batch[\"labels\"]\n    ]\n\n    return batch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T12:02:26.459159Z","iopub.execute_input":"2025-11-08T12:02:26.459453Z","iopub.status.idle":"2025-11-08T12:02:31.222425Z","shell.execute_reply.started":"2025-11-08T12:02:26.459431Z","shell.execute_reply":"2025-11-08T12:02:31.221642Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0eb1045dab844fed8ab34e9a766a9ec0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"62ab6ae7c85945bb953a9aa1d548ee42"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b02cc44da1b4eacb4ac10c3c430533b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1370d59a9b574dda8530d7781d8c6145"}},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"# Step 8: Apply preprocessing\nencoded_dataset = dataset.map(\n    preprocess_function,\n    batched=True,\n    batch_size=32,\n    remove_columns=[\"article\", \"highlights\"]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T12:02:39.108819Z","iopub.execute_input":"2025-11-08T12:02:39.109792Z","iopub.status.idle":"2025-11-08T12:20:24.615342Z","shell.execute_reply.started":"2025-11-08T12:02:39.109767Z","shell.execute_reply":"2025-11-08T12:20:24.614550Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/74063 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ffd0a1026d534f629ec897bb093762f9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/9258 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"724a04f5527c4747bf6a3545423f572b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/9258 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc50d866f3bd4fd0be7a48a8cca40dc6"}},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"# Step 9: Set format for PyTorch\nencoded_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 10: Verify the processed dataset\nprint(\"Processed dataset structure:\", encoded_dataset)\nprint(\"Sample input IDs:\", encoded_dataset[\"train\"][0][\"input_ids\"])\nprint(\"Sample attention mask:\", encoded_dataset[\"train\"][0][\"attention_mask\"])\nprint(\"Sample labels:\", encoded_dataset[\"train\"][0][\"labels\"])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save processed dataset\nencoded_dataset.save_to_disk(\"/kaggle/working/processed_cnn_stories\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}