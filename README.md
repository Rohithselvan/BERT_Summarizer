# ğŸ§  BERT Summarizer â€“ GPU Fine-Tuned Model

An intelligent **text summarization system** built using a fine-tuned **BERT Encoderâ€“Decoder model**.  
This project performs **abstractive summarization** on long text passages, trained on a **custom NLP dataset**.  

ğŸª„ **Key Highlights**
- Fine-tuned **BERT2BERT** model using PyTorch and Hugging Face Transformers  
- Datasets hosted on **Kaggle**  
- Final trained model hosted on **Hugging Face** for instant use  
- Supports GPU-accelerated inference for faster results  

ğŸ“ **Resources**
- **Model:** [Rohith1872/bert-summarizer-gpu-final](https://huggingface.co/Rohith1872/bert-summarizer-gpu-final)  
- **Raw Dataset:** [Kaggle â€“ NLP Project](https://www.kaggle.com/datasets/yoo1234567/nlp-proj)  
- **Processed Dataset:** [Kaggle â€“ Processed Dataset](https://www.kaggle.com/datasets/yoo1234567/dataset)

---

> ğŸ’¡ *Clone the repo, open the notebooks in Kaggle, or directly load the Hugging Face model to start summarizing your own text.*

---

## ğŸ“˜ Overview

This project demonstrates a **BERT Encoderâ€“Decoder** model fine-tuned for **text summarization**.  
It uses `bert-base-uncased` as both encoder and decoder, fine-tuned on a custom dataset in a Kaggle GPU environment.  
The model is now available for public inference on Hugging Face.

---

## ğŸ“‚ Project Structure
```
ğŸ“ BERT-Summarizer/
â”œâ”€â”€ ğŸ““ 1_dataset_preprocessing.ipynb
â”œâ”€â”€ ğŸ““ 2_model_training.ipynb
â”œâ”€â”€ ğŸ““ 3_model_testing.ipynb
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â””â”€â”€ .gitignore
```


| File | Description |
|------|--------------|
| **1_dataset_preprocessing.ipynb** | Cleans and preprocesses the raw dataset. |
| **2_model_training.ipynb** | Fine-tunes the BERT encoderâ€“decoder model. |
| **3_model_testing.ipynb** | Loads the model from Hugging Face and generates summaries. |

---

## ğŸ“Š Datasets

### ğŸ”¹ Raw Dataset
Base data used for model training.  
ğŸ‘‰ [**NLP Project Raw Dataset**](https://www.kaggle.com/datasets/yoo1234567/nlp-proj)

### ğŸ”¹ Processed Dataset
Cleaned, tokenized, and ready for model input.  
ğŸ‘‰ [**Processed Dataset**](https://www.kaggle.com/datasets/yoo1234567/dataset)

---

## ğŸ¤– Model Information

The fine-tuned summarization model is hosted on **Hugging Face**:

ğŸ‘‰ [**Rohith1872/bert-summarizer-gpu-final**](https://huggingface.co/Rohith1872/bert-summarizer-gpu-final)

### ğŸ”¸ Model Architecture
- **Base Model:** `bert-base-uncased`  
- **Type:** Encoderâ€“Decoder (BERT2BERT)  
- **Training Platform:** Kaggle GPU (Tesla T4)  
- **Framework:** PyTorch + Hugging Face Transformers  

---

## âš™ï¸ Requirements

Create a `requirements.txt` with:

torch>=2.0.0
transformers>=4.35.0
huggingface_hub>=0.23.0
pandas>=2.0.0
numpy>=1.25.0
tqdm
nltk
scikit-learn
datasets

go
Copy code

Install dependencies:

```bash
pip install -r requirements.txt
ğŸš€ Quick Usage (Summarization Demo)
python
Copy code
from transformers import BertTokenizer, EncoderDecoderModel
import torch

# Load model and tokenizer
model_name = "Rohith1872/bert-summarizer-gpu-final"
tokenizer = BertTokenizer.from_pretrained(model_name)
model = EncoderDecoderModel.from_pretrained(model_name)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

text = """
Artificial Intelligence is transforming industries worldwide. From healthcare
and education to finance and transportation, AI enhances efficiency and reduces
costs. However, without proper regulation, it may cause job loss and privacy concerns.
"""

inputs = tokenizer(text, return_tensors="pt", truncation=True, padding="max_length", max_length=512)
inputs = {k: v.to(device) for k, v in inputs.items()}

with torch.no_grad():
    summary_ids = model.generate(
        inputs["input_ids"],
        max_length=150,
        num_beams=4,
        no_repeat_ngram_size=2,
        early_stopping=True,
        top_p=0.95,
        top_k=50
    )

summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
print("\nğŸ“ Summary:", summary)
ğŸ§© Training Workflow
Preprocessing:
Load and clean the raw dataset (remove stopwords, normalize text, prepare input-output pairs).

Training:
Fine-tune BERT using the processed dataset for abstractive summarization.

Testing:
Evaluate the model and generate human-readable summaries.

Deployment:
Upload final model and tokenizer to Hugging Face using push_to_hub().

ğŸ“ˆ Results
Metric	Description	Score
ROUGE-1	Overlap of unigrams	High (Good coherence)
ROUGE-L	Longest common subsequence	Stable performance
Inference Speed	~1.2 s per paragraph (on GPU)	

(Exact metrics depend on dataset split and training parameters.)

ğŸ§  Key Features
Custom fine-tuned BERT2BERT model for abstractive summarization

Preprocessed dataset for reproducible training

GPU-optimized for fast experimentation

Public Hugging Face model for open use

ğŸ§° Environment
Platform: Kaggle

Runtime: GPU (Tesla T4)

Language: Python 3.11

Framework: PyTorch, Hugging Face Transformers

ğŸ‘¤ Author
Rohith Selvan
ğŸ“§ rohithselvan10@gmail.com
ğŸŒ Hugging Face Profile

ğŸ License
This project is released under the MIT License.
Feel free to use, modify, and share with attribution.

ğŸ“œ Citation
latex
Copy code
@misc{rohith1872_bert_summarizer_2025,
  author = {Rohith Selvan},
  title  = {BERT Summarizer (GPU Fine-Tuned)},
  year   = {2025},
  howpublished = {\url{https://huggingface.co/Rohith1872/bert-summarizer-gpu-final}}
}
âœ… Clone the repository, open the notebooks on Kaggle, or use the Hugging Face model directly for instant summarization.

yaml
Copy code

---

### ğŸ”§ Quick Tips for Upload
1. Save this file as **`README.md`** in your main project folder.  
2. Add `.gitignore` and `requirements.txt` (from earlier).  
3. Run:
   ```bash
   git add .
   git commit -m "Added final README and structure"
   git push origin main
Your GitHub repo will now look clean, structured, and professional.
